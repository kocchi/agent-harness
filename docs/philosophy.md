# 設計原理（Philosophy）

agent-harness の設計原理を解説します。

## P0: セッション開始時の検査

**要点**: セッション開始時に rules-validator を自動実行する

各セッションの開始時に、ルールの整合性を自動検査します。
これにより、ルールの逸脱を早期に検出し、一貫性を保ちます。

**ルール**:
- セッション開始時に rules-validator サブエージェントを実行する
- ルールの整合性をチェックする
- 問題があれば警告を出す

## P1: コンテキスト分離

**要点**: 役割ごとにエージェントを分け、単一エージェントに全文脈を詰め込まない

AI活用が進むほど、単一エージェントの巨大コンテキストは破綻しやすくなります。
よって、役割ごとにサブエージェントを分離し、コンテキストを侵食し合わない構造で品質と速度を上げます。

**ルール**:
- 単一エージェントに全文脈を詰め込まない
- 役割ごとにサブエージェントを分離する
- サブエージェント間は成果物（中間生成物）で受け渡す
- 並列稼働を前提とし、主コンテキストを侵食しない

**アンチパターン**:
- 一つのプロンプトに全情報を詰め込む
- エージェントの役割が曖昧なまま進める

## P2: 中間生成物で合意を運ぶ

**要点**: 契約・失敗モード・テスト戦略で「意図と根拠」を検査可能にする

Righting Software の考え方を踏襲し、コードレビューだけでなく、中間生成物で意図と根拠を検査可能にします。
曖昧→抽象合意→具体化→実装、という往復運動を成果物で管理します。

**中間生成物**:
- **契約**: API/イベント/データの入出力仕様
- **失敗モード**: 何が壊れうるか、その影響と対策
- **テスト戦略**: 何が証明書になるか、どのレベルで検証するか
- **計測設計**: 成功/失敗を何で判定するか

**ルール**:
- コードの前に中間生成物を作成する
- レビューはコードだけでなく中間生成物も対象とする
- 意図と根拠を検査可能な形で残す

## P3: 道具を減らす

**要点**: 各スライスで「減らせる道具」を必ず評価する

リアーキテクチャは段階移行を基本とし、各スライスごとに「減らせる道具／消せる構成」を評価します。
統合すべきか分割すべきかの判断も、エージェントが検査・提案できるようにします。

**削除の条件**:
- 削除対象が明確である
- 代替経路が証明されている（新経路のテストが通っている）
- ロールバックがある（フラグ/段階展開/互換性）
- 影響範囲が見えている（契約と依存が書かれている）

## P4: 知識は資産

**要点**: ルール/スキル/ナレッジとして蓄積し、以後の作業に自動適用する

環境構築の暗黙知、結合の作法、チーム固有の留意点などは、都度チャットで消費せず、ルール／スキル／ナレッジとして蓄積し、以後の作業に自動適用される仕組みにします。

**知識の種類**:
- **ルール**: 常に適用される制約や作法（例: コミットメッセージの形式）
- **スキル**: 特定の作業を行う手順や知識（例: 環境構築手順）
- **ナレッジ**: 参照用の情報や過去の学習（例: 失敗事例）

**ルール**:
- 知見はチャットで消費せず、ルール/スキル/ナレッジとして蓄積する
- 蓄積した知識は以後の作業に自動適用される
- SSoTに集約し、ダブルメンテを禁止する

## P5: ペルソナ注入

**要点**: 著名人やステークホルダーの視点を借りて、判断の質を上げる

単一 AI は自己矛盾を解消しようとして平凡化します。
著名人の思考パターンを注入することで、一貫した判断軸を維持できます。

**効果**:
- 判断が必要なタスクで効果が高い（戦略、設計方針）
- 実行タスクでは効果がない（単純実装）
- 名前だけでも効果あり、名前+哲学で最大効果

**ステークホルダー憑依**:
- 特定のステークホルダーの視点を AI に注入
- 「この人ならどう判断するか」を代行
- レビューの負荷軽減、コミットメントの確認

### Agent の構成（Role / Persona / Skill）

Agent は**器**として、以下を注入できる:

| 概念 | 定義 | 注入タイミング |
|------|------|---------------|
| **Role** | 責務（何をするか） | Agent に割り当て |
| **Persona** | 人格・視点（どう考えるか） | 判断が必要なとき |
| **Skill** | 手順書（どうやるか） | 手順が必要なとき |

**組み合わせの例**:
- マーケティングが得意な EM（Role）としての Steve Jobs（Persona）に slice スキルを注入
- TechLead（Role）に Kent Beck（Persona）を注入してテスト戦略を策定

**Persona の取得方法**:
- **文脈から導出**: persona-vessel が context から「歪み」を導出し、LLM 内部知識から人格を選出（事前定義リストは参照しない）
- **カスタム定義**: `.cursor/personas/custom/` にユーザーが定義

### 場におけるペルソナ運用

**メインエージェントには憑依させない**:
- メインはオーケストレーター。中立を保つ。
- 憑依はサブエージェントにのみ行う。

**サブエージェントには複数人を憑依させる**:
- 場（Sprint Planning, Review, Retro など）は同期的に実行。
- 複数人格が議論する必要があるため、1サブエージェントに複数人を憑依させる。
- 人格リストは固定せず、「召喚条件」を記載。LLM の内部知識を最大限活用。

**議論の過程を残す**:
- 誰が何を言い、どう食い違ったか、どう統合したかを記録する。
- 人間の学習につながる。

**ペルソナ選定の変数**:
- 場に入る前に、対象を分析する（例: スプリントの focus, risk, context）。
- 分析結果を変数として、召喚条件を生成する。

## P6: 相互監視

**要点**: 最低2人（理想3人）の異なる視点で盲点をカバーする

一貫した判断基準は一貫した盲点を持ちます。
異なるメンタルモデルを持つ複数 AI で相互監視することで、見落としを防ぎます。

**構成**:
- 最低2人、理想は3人
- 異なる失敗パターンを持つこと
- 固定編成ではなく、タスクに応じて変える

**効果**:
- エラー検出数が増加（単一 AI の 1.7 倍）
- 再考ラウンドで判断が変わることがある

## P7: 契約で価値をつなぐ

**要点**: Product Goal → Sprint Goal → Increment を契約でつなぎ、価値を一貫させる

スクラムガイド 2020 の Product Goal / Sprint Goal を契約として明文化し、
価値定義から成果物まで一貫した検証を可能にします。

**契約の階層**:
- **Product Goal Contract**: プロダクトの長期目標（複数スプリントにまたがる）
- **Sprint Goal Contract**: スプリントの短期目標（1スプリント）
- **Increment Contract**: 成果物と完了条件（DoD）

**価値の分類**（Slice Contract）:
- **必須**: なければ価値がない（実装前に確認）
- **既知**: 作り方がわかっている（実装後に確認）
- **未知**: 検証が必要（最初の Increment で検証）

**Sprint 0**:
公式スクラムではないが、複数の「どう作るか」を発散的に試す余白として使う。
how-experiment で実装方針を選定する。

---

## 反パターン

避けるべき行動や設計を明示します。

- ❌ 一人のAIに全てを任せる（凡庸化、暴走）
- ❌ 一つのプロンプトに全情報を詰め込む
- ❌ コードを書いてからテストを考える
- ❌ AIが自己バランシングして尖りを失う
- ❌ 編成表を作ってメンテナンスが必要になる
- ❌ 前提を問い直すことを目的化する
- ❌ 妥協案で丸くする（尖りを失う統合）
- ❌ 問いを投げっぱなしにする（提案しない）
- ❌ 入力（レポート、提案）を契約と混同する
- ❌ 場を経由せずに合意があると思い込む

---

## 守破離の進化

agent-harness の成熟度モデルを示します。

### 守（Phase 1）: 型を学ぶ

- スクラムのプラクティスを忠実に実装した型を提供
- 中間生成物のフォーマット（契約、テスト戦略）
- チェックポイントの配置
- AIの役割: 型を提示し、逸脱を検知する
- 検証: 型が機能するか

### 破（Phase 2）: 型を破る

- 状況に応じて型をカスタマイズ
- 「最終判断」と「それ以外」の境界が明確になる
- 確認から監督へ、監督から例外介入へ
- AIの役割: 学びに基づいて改善を提案する
- 検証: カスタマイズが効果的か

### 離（Phase 3）: 型から離れる

- 「協働」という概念自体が不要になる
- 人間は最終判断のみを引き取る
- AIは自律的に遷移パターンを学習する
- 究極: 最終判断以外は全自動
- 検証: 自律が機能するか

---

## 最終判断：人間の役割

AIが代替できない、人間が引き取るべき判断を明示します。

| ペルソナ | 最終判断の例 |
|---------|-------------|
| Product Owner | Go/No-Go、撤退判断 |
| Tech Lead | 技術的な不可逆判断（アーキテクチャ選択、技術負債の受容） |
| Engineer | 新規領域の探索（前例のない実装への挑戦） |
| Scrum Master | チームの健全性の判断（人間的な側面） |
| Product Manager | 仮説の賭けを引き受ける |

**究極形**:
- AIが自律的に判断・実行・学習
- 人間は技術的制約を埋める役割のみ
- 技術的制約が解消されれば、人間の役割も解消される
- 「最終判断」もAIが持てるならそれでいい

---

## 多視点セッション

異なるメンタルモデルを持つサブエージェント同士で提案を応酬し、統合提案を生み出します。

**流れ**:
1. 各自がベストな提案を出す
2. 提案をぶつけ合う
3. お互いの盲点に気づく
4. 再提案 / 反論
5. 統合提案を出す

**アウトカム**:
- 統合提案（根拠付き）
- 各視点の盲点がどうカバーされたか

---

## 組織固有の原理

P8 以降は組織固有の原理として定義できます。
例: モバイル領域の統合、セキュリティポリシー、コンプライアンス要件など
